\subsection{Qualitätsmaße}\label{sec:measures}
%
%	\begin{itemize}
%	\item Successful session \citep{hb_18,Smyth05alive-user}
%	\item Precision /Recall /F1, \citep{rs}[Kap. 7]
%	\item User- und Itemcoverage \citep{rs}[S. 183]
%	\item Conversion / Click-through rates -> Joachim05 \\
%	Clickthrough ist als absolutes Maß eher problematisch da der Nutzer zum einen beeinflusst wird durch das Vertrauen in die Suchmaschine und durch die Qualität der sonst angebotenen Suchergebnisse. Bei einem Klick in einer Suchergebnisliste ist es deshalb sinnvoller die Relevanz abhängig von den nicht geklickten Elementen zu messen. (Joachim05)
%	\end{itemize}
%
%Zusammenfassung aus Cacheda11, Cremonesi10, Joachim05 und Herlocker04

Im folgenden Abschnitt werden verschiedene Wege zur Bewertungen von Empfehlungsdiensten beschrieben. Sie dienen zum qualitativen Vergleich von Filtermodellen und als Kontrollwerkzeug vor und während der Inbetriebnahme des Dienstes.

\subsubsection{Mittlere Abweichung} \label{sec:qa_rmse}

Eine naheliegende Methode zum Vergleich von Filtermodellen ist es, deren Fehlerquote in Bezug zu vorliegenden historischen Nutzerdaten zu bestimmen, bzw. zu messen wie stark die ermittelten Werte von den tatsächlichen abweichen. Hierfür wird oft der \textit{mittlere absolute Fehler} (engl. \acf{MAE}) oder die \textit{Wurzel aus dem mittleren quadratischen Fehler} (engl. \acf{RMSE}) genutzt.

Auf der Basis einer Testmenge $\mathcal{T}$ wird für alle darin befindlichen Nutzer-Element-Paare $(u,i)$ der berechnete Ratingwert $\hat{r}_{ui}$ mit dem tatsächlichen $r_{ui}$ vergleichen. Die beiden Fehlermaße werden dann wie folgt gebildet:
\begin{align}
MAE & = \sqrt{\frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}}{|\hat{r}_{ui}-r_{ui}|}} \label{form:mae} \\
RMSE & = \sqrt{\frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}}{(\hat{r}_{ui}-r_{ui})^2}} \label{form:rmse}
\end{align}

Der Unterschied zwischen den Methoden liegt in der Bewertung großer Fehler. Wie in Tabelle \ref{tab:mae-rmse} gezeigt wird, kann dies durchaus ausschlaggebend sein. Im gezeigten Beispiel würde das Modell $\hat{r}_{(1)}$ vom \acs{MAE} bevorzugt, weil ein Fehler durch die drei richtigen Werte stärker ausgeglichen wird. Beim Vergleich mit dem \acs{RMSE} würde Modell $\hat{r}_{(2)}$ bevorzugt weil die vier ``kleinen'' Fehler in $\hat{r}_{(2)}$ weniger ins Gewicht fallen als der größere Fehler in $\hat{r}_{(1)}$. \citep{hb_08}
\begin{SCtable}
  \centering
  \begin{tabular}{ | r | p{1cm} | p{1cm} | p{1cm} | }
    \hline
    & $r$ & $\hat{r}_{(1)}$ & $\hat{r}_{(2)}$ \\ \hline
    Item1 & 3 & 3 & \underline{2} \\
    Item2 & 4 & \underline{1} & \underline{2} \\
    Item3 & 2 & 2 & \underline{3} \\
    Item4 & 3 & 3 & \underline{2} \\ \hline \hline
    MAE & & \textbf{0.75} & 1.25 \\
    RMSE & & 1.5 & \textbf{1.32} \\
    \hline
  \end{tabular}
  \caption[Beispielrechnung zum Vergleich von \acs{MAE} und \acs{RMSE}]{\footnotesize Beispielrechnung zum Vergleich von \acs{MAE} und \acs{RMSE} - gegenübergestellt sind tatsächliche Bewertungen $r$ und von zwei verschiedenen Quellen erzeugte angenommene Bewertungen $\hat{r}_{(1)}$ und $\hat{r}_{(2)}$. Abweichende Bewertungen sind unterstrichen. }
  \label{tab:mae-rmse}
\end{SCtable}
%RMS

\subsubsection{Trefferquote und Genauigkeit}
% Fall-Out

Wird eine Liste von Empfehlungen erzeugt, kann neben der möglichen Bewertung und deren Genauigkeit auch gemessen werden, ob und wieviele der Empfehlungen korrekt waren. Bei der Evaluierung mit historischen Daten werden hierzu aus einem Nutzerprofil eine Reihe von Elementen entfernt und für das abgeleitete Profil eine Liste von Empfehlungen erzeugt. Durch den Vergleich der entfernten Elemente mit den Empfehlungen können dann \textit{Genauigkeit} $P$ (engl. Precision) und \textit{Trefferquote} $R$ (engl. Recall) wie folgt gemessen werden:
\begin{align}
P_u & = \frac{|hits_u|}{| recset_u | } \label{form:precision} \\
R_u & = \frac{|hits_u|}{ | testset_u | } \label{form:recall}
\end{align}

Die Anzahl der korrekt empfohlenen Elemente $hits_u$ wird zur Anzahl aller empfohlenen Elemente $recset_u$ bzw. zur Anzahl der möglichen richtigen Empfehlungen $testset_u$ ins Verhältnis gesetzt. Die Verbesserung beider Werte steht dabei oft im Widerspruch zueinander. So kann durch die Vergrößerung der Anzahl von empfohlenen Elementen auch die Wahrscheinlichkeit von Treffern und damit die Trefferquote gesteigert werden. Allerdings hätte die insgesamt gestiegene Zahl von Empfehlungen auch einen negativen Einfluss auf die Genauigkeit. Zur Kombination beider Werte wird daher oft die $F_1$ Metrik genutzt. Sie gewichtet beide Werte mit einer Neigung zum schlechteren.
\begin{align}
F_1 & = \frac{2 * P * R}{P+R} \label{form:f1}
\end{align}

Die ebenfalls mögliche Bestimmung der Ausfallquote (engl. Fallout), also der Anzahl der nicht korrekt empfohlenen Elemente im Verhältnis zur Menge aller Elemente, wird seltener zur Bewertung von Ergebnissen herangezogen. \citep{hb_08,rs}

\subsubsection{Empirische Messung}\label{sec:measure_c}

Ergänzend zur Messung der Approximationsgüte des erzeugten Modells muss bei einem praktischen Einsatz auch der tatsächliche Erfolg der eingesetzten Methode gemessen werden. Dafür bieten sich die aus dem Marketing stammenden \textit{Klickraten} und \textit{Conversion-Raten} Messung an, des weiteren kann in Verbindung mit Suchindexen auch die Bestimmung der \textit{Successful Session} Quote genutzt werden.

\paragraph{Klickrate} Beschreibt die Anzahl der Klicks auf ein Element, zum Beispiel einen Werbebanner, im Verhältnis zur Häufigkeit mit der es angezeigt wurde. 

\paragraph{Conversion-Rate} Mit der Conversion-Rate wird bestimmt welcher Anteil der Besucher ein bestimmtes Ziel, die sog. \textit{Conversion}, erreicht haben. Mögliche Ziele sind zum Beispiel erfolgreiche Kaufabschlüsse oder Kontaktaufnahmen der Kunden. Die Ziele werden abhängig vom Kontext so gewählt, dass diese aktiv zur Wertschöpfung beitragen. Die Berechnung der Conversion-Rate entspricht gem. Formel (\ref{form:cr}) dem Verhältnis der gemessenen Anzahl an Conversions $conv$ und der Anzahl der Besucher $visitors$ die im gegebenen Zeitraum eine oder mehrere Seiten aufgerufen haben. Maschinelle Seitenzugriffe, zum Beispiel durch Suchmaschinen-Spider, fließen nicht in Messung der Nutzerzahlen ein. Ebenso werden wiederholte Conversions des gleichen Besuchers nicht mehrfach gezählt. \citep{krueger2011conversion}
\begin{align}
CR & = \frac{|conv|}{|visitors| } * 100 \label{form:cr}
\end{align}

\paragraph{Successful Session}\label{sec:successfulsession} Da die Nutzeroberfläche von Suchmaschinen in der Regel kein explizites Feedback vom Nutzer zur Qualität der gefundenen Ergebnisse  zulässt, muss die Relevanz einer Ergebnisliste über implizite Verfahren gemessen werden. In \citep{smyth05a} wird die Methode der \textit{Successful Session} für diesen Zweck beschrieben. Die Ergebnisliste einer Suche wird demnach immer dann als relevant bewertet, wenn der Nutzer mindestens eines der Ergebnisse ausgewählt hat. Wird keines gewählt, wird die Liste entsprechend als irrelevant bewertet. Eine weitere Abstufung bzw. Steigerung der Relevanz, zum Beispiel bei wiederholter Ergebniswahl, wird nicht getroffen. \citep{smyth05a} \todo[color=green]{HB Kap. 10.4.2 dazu?}
%\subsubsection{Normalisierter DCG}
%\subsubsection{A/B Testing}